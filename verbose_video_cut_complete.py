#!/usr/bin/env python3
"""
Optimized Precise Cut Point Video Matcher with Multiple Detection Methods
Extracts frames at cut boundaries and finds exact matches in original video
WITH ENHANCED VERBOSE OUTPUT
"""

import cv2
import numpy as np
import os
import sys
from datetime import timedelta
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time

def install_requirements():
    """Install required packages if not available"""
    try:
        import cv2
        import numpy as np
        print("‚úÖ Required packages already installed")
    except ImportError:
        print("Installing required packages...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "opencv-python", "numpy"])
            print("‚úÖ Packages installed successfully")
        except Exception as e:
            print(f"‚ùå Failed to install packages: {e}")
            print("Please run manually: pip3 install opencv-python numpy")
            sys.exit(1)

def enhanced_perceptual_hash(frame, cache=None):
    """Create enhanced perceptual hash with multiple features and optional caching"""
    if cache is not None:
        frame_key = hash(frame.tobytes())
        if frame_key in cache:
            return cache[frame_key]
    
    # Resize and convert to grayscale
    resized = cv2.resize(frame, (32, 32))
    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
    
    # Feature 1: Basic perceptual hash
    mean = np.mean(gray)
    basic_hash = gray > mean
    
    # Feature 2: Edge detection hash
    edges = cv2.Canny(gray, 50, 150)
    edge_hash = edges > 0
    
    # Feature 3: Histogram features (reduced to key bins)
    hist = cv2.calcHist([gray], [0], None, [16], [0, 256])
    hist_features = hist.flatten() / np.sum(hist)  # Normalize
    
    result = {
        'basic': basic_hash.flatten(),
        'edges': edge_hash.flatten(),
        'hist': hist_features
    }
    
    if cache is not None:
        cache[frame_key] = result
    
    return result

def compare_enhanced_frames(hash1, hash2):
    """Compare two enhanced frame hashes with weighted similarity"""
    # Basic hash similarity
    basic_sim = np.sum(hash1['basic'] == hash2['basic']) / len(hash1['basic'])
    
    # Edge hash similarity
    edge_sim = np.sum(hash1['edges'] == hash2['edges']) / len(hash1['edges'])
    
    # Histogram similarity (using correlation)
    hist_sim = np.corrcoef(hash1['hist'], hash2['hist'])[0, 1]
    if np.isnan(hist_sim):
        hist_sim = 0
    hist_sim = (hist_sim + 1) / 2  # Normalize to 0-1
    
    # Weighted combination
    combined_similarity = (basic_sim * 0.4 + edge_sim * 0.3 + hist_sim * 0.3)
    return combined_similarity, basic_sim, edge_sim, hist_sim

def detect_cut_points_adaptive(video_path, initial_threshold=0.03, max_segments=50):
    """Adaptive cut detection that adjusts threshold based on results"""
    print(f"üîç Starting adaptive cut detection...")
    print(f"   Video: {os.path.basename(video_path)}")
    print(f"   Initial threshold: {initial_threshold}")
    print(f"   Max segments: {max_segments}")
    
    start_time = time.time()
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise Exception(f"Could not open video: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    
    print(f"üìä Video info:")
    print(f"   FPS: {fps:.2f}")
    print(f"   Total frames: {total_frames:,}")
    print(f"   Duration: {timedelta(seconds=int(duration))}")
    
    # First pass: collect similarity data with frame skipping for speed
    print("\nüîç Starting quick analysis pass...")
    similarities = []
    frame_positions = []
    prev_hash = None
    frame_count = 0
    skip_frames = max(1, total_frames // 5000)  # Sample ~5000 frames max
    
    print(f"   Skip frames: {skip_frames} (analyzing every {skip_frames} frames)")
    
    hash_cache = {}  # Cache for frame hashes
    last_progress_time = time.time()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # Skip frames for faster analysis
        if frame_count % skip_frames != 0:
            frame_count += 1
            continue
        
        current_hash = enhanced_perceptual_hash(frame, hash_cache)
        
        if prev_hash is not None:
            similarity, _, _, _ = compare_enhanced_frames(prev_hash, current_hash)
            similarities.append(similarity)
            frame_positions.append(frame_count)
        
        prev_hash = current_hash
        frame_count += 1
        
        # More frequent progress updates
        current_time = time.time()
        if current_time - last_progress_time > 2.0 or frame_count % 1000 == 0:
            progress = (frame_count / total_frames) * 100
            elapsed = current_time - start_time
            fps_analysis = frame_count / elapsed if elapsed > 0 else 0
            print(f"   Quick analysis: {progress:.1f}% ({frame_count:,}/{total_frames:,}) | {fps_analysis:.1f} fps | {len(similarities)} comparisons")
            last_progress_time = current_time
    
    cap.release()
    
    analysis_time = time.time() - start_time
    print(f"‚úÖ Quick analysis complete in {analysis_time:.1f}s")
    
    if not similarities:
        print("‚ùå No similarities calculated")
        return [0.0, total_frames / fps], fps
    
    # Calculate adaptive threshold
    similarities = np.array(similarities)
    percentiles = [1, 3, 5, 10, 15, 20]
    
    print(f"\nüìä Similarity analysis (from {len(similarities)} comparisons):")
    print(f"   Min: {np.min(similarities):.4f}")
    print(f"   Max: {np.max(similarities):.4f}")
    print(f"   Mean: {np.mean(similarities):.4f}")
    print(f"   Std: {np.std(similarities):.4f}")
    
    for p in percentiles:
        thresh = np.percentile(similarities, p)
        potential_cuts = np.sum(similarities < thresh)
        print(f"   {p:2d}th percentile: {thresh:.4f} ({potential_cuts:3d} potential cuts)")
    
    # Choose threshold that gives reasonable number of cuts
    chosen_threshold = None
    for p in percentiles:
        thresh = np.percentile(similarities, p)
        potential_cuts = np.sum(similarities < thresh)
        if 5 <= potential_cuts <= max_segments:
            chosen_threshold = thresh
            print(f"‚úÖ Using {p}th percentile threshold: {chosen_threshold:.4f}")
            break
    
    if chosen_threshold is None:
        # If no good threshold found, use a more aggressive one
        chosen_threshold = np.percentile(similarities, 10)
        print(f"‚ö†Ô∏è  Using 10th percentile as fallback: {chosen_threshold:.4f}")
    
    # Second pass: detect cuts with chosen threshold
    print(f"\nüîç Starting detailed cut detection with threshold {chosen_threshold:.4f}...")
    return detect_cut_points_method1(video_path, chosen_threshold)

def detect_cut_points_method2(video_path, drop_threshold=0.15, base_threshold=0.05):
    """Method 2: Detect cuts using similarity drops and rolling average"""
    print(f"\nüîç METHOD 2: Similarity drops detection")
    print(f"   Drop threshold: {drop_threshold}")
    print(f"   Base threshold: {base_threshold}")
    
    start_time = time.time()
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise Exception(f"Could not open video: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"üìä Processing {total_frames:,} frames at {fps:.2f} fps...")
    
    cut_points = [0.0]  # Always include start
    prev_hash = None
    prev_similarity = None
    frame_count = 0
    similarity_window = []  # Rolling window for adaptive threshold
    window_size = 50
    
    hash_cache = {}
    last_progress_time = time.time()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print(f"   üìπ Reached end of video at frame {frame_count}")
            break
        
        current_hash = enhanced_perceptual_hash(frame, hash_cache)
        
        if prev_hash is not None:
            similarity, basic_sim, edge_sim, hist_sim = compare_enhanced_frames(prev_hash, current_hash)
            similarity_window.append(similarity)
            
            # Keep window size manageable
            if len(similarity_window) > window_size:
                similarity_window.pop(0)
            
            # Calculate adaptive threshold
            if len(similarity_window) >= 10:
                window_avg = np.mean(similarity_window)
                adaptive_threshold = max(base_threshold, window_avg - 0.3)
            else:
                adaptive_threshold = base_threshold
            
            # Method 2 detection: Look for significant drops OR very low similarity
            is_cut = False
            cut_reason = ""
            
            if similarity < adaptive_threshold:
                is_cut = True
                cut_reason = f"low_sim({similarity:.3f} < {adaptive_threshold:.3f})"
            
            if prev_similarity is not None and (prev_similarity - similarity) > drop_threshold:
                is_cut = True
                cut_reason += f" drop({prev_similarity:.3f} -> {similarity:.3f})"
            
            if is_cut:
                timestamp = frame_count / fps
                # Avoid duplicate cuts too close together
                if not cut_points or (timestamp - cut_points[-1]) > 2.0:
                    cut_points.append(timestamp)
                    print(f"üé¨ CUT DETECTED at {timedelta(seconds=int(timestamp))} - {cut_reason}")
                    print(f"   Frame {frame_count:,} | Basic: {basic_sim:.3f} | Edge: {edge_sim:.3f} | Hist: {hist_sim:.3f}")
            
            # Verbose output every 1000 frames
            current_time = time.time()
            if frame_count % 1000 == 0 or current_time - last_progress_time > 3.0:
                window_avg = np.mean(similarity_window) if similarity_window else 0
                progress = (frame_count / total_frames) * 100
                elapsed = current_time - start_time
                fps_analysis = frame_count / elapsed if elapsed > 0 else 0
                
                print(f"   Frame {frame_count:,}: {progress:.1f}% | sim={similarity:.3f} | window_avg={window_avg:.3f} | thresh={adaptive_threshold:.3f} | {fps_analysis:.1f} fps")
                last_progress_time = current_time
            
            prev_similarity = similarity
        
        prev_hash = current_hash
        frame_count += 1
    
    cap.release()
    
    # Add end time
    duration = total_frames / fps
    cut_points.append(duration)
    
    elapsed = time.time() - start_time
    print(f"‚úÖ Method 2 complete in {elapsed:.1f}s")
    print(f"   Result: {len(cut_points) - 1} segments with {len(cut_points) - 2} cut points")
    return cut_points, fps

def detect_cut_points_method3(video_path, percentile_threshold=5):
    """Method 3: Statistical approach - detect cuts based on percentile of all similarities"""
    print(f"\nüîç METHOD 3: Statistical detection")
    print(f"   Using {percentile_threshold}th percentile threshold")
    
    start_time = time.time()
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise Exception(f"Could not open video: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"üìä Processing {total_frames:,} frames...")
    
    # First pass: collect all similarities
    print("üîç First pass: collecting similarity data...")
    similarities = []
    timestamps = []
    prev_hash = None
    frame_count = 0
    hash_cache = {}
    last_progress_time = time.time()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print(f"   üìπ Reached end of video at frame {frame_count}")
            break
        
        current_hash = enhanced_perceptual_hash(frame, hash_cache)
        
        if prev_hash is not None:
            similarity, _, _, _ = compare_enhanced_frames(prev_hash, current_hash)
            similarities.append(similarity)
            timestamps.append(frame_count / fps)
        
        prev_hash = current_hash
        frame_count += 1
        
        current_time = time.time()
        if frame_count % 1000 == 0 or current_time - last_progress_time > 3.0:
            progress = (frame_count / total_frames) * 100
            elapsed = current_time - start_time
            fps_analysis = frame_count / elapsed if elapsed > 0 else 0
            print(f"   First pass: {progress:.1f}% ({frame_count:,}/{total_frames:,}) | {fps_analysis:.1f} fps | {len(similarities)} comparisons")
            last_progress_time = current_time
    
    cap.release()
    
    # Calculate statistical threshold
    threshold = np.percentile(similarities, percentile_threshold)
    print(f"\nüìä Statistical analysis:")
    print(f"   Calculated threshold: {threshold:.3f} ({percentile_threshold}th percentile)")
    print(f"   Similarity range: {np.min(similarities):.3f} to {np.max(similarities):.3f}")
    print(f"   Mean similarity: {np.mean(similarities):.3f}")
    
    # Second pass: find cuts
    print(f"\nüîç Finding cuts below threshold {threshold:.3f}...")
    cut_points = [0.0]
    cuts_found = 0
    
    for i, (sim, timestamp) in enumerate(zip(similarities, timestamps)):
        if sim < threshold:
            # Avoid cuts too close together
            if not cut_points or (timestamp - cut_points[-1]) > 2.0:
                cut_points.append(timestamp)
                cuts_found += 1
                print(f"üé¨ Cut {cuts_found} at {timedelta(seconds=int(timestamp))} (similarity: {sim:.3f})")
    
    # Add end time
    duration = total_frames / fps
    cut_points.append(duration)
    
    elapsed = time.time() - start_time
    print(f"‚úÖ Method 3 complete in {elapsed:.1f}s")
    print(f"   Result: {len(cut_points) - 1} segments with {len(cut_points) - 2} cut points")
    return cut_points, fps

def detect_cut_points_method1(video_path, threshold=0.03):
    """Method 1: Detect cut points using absolute similarity threshold (optimized)"""
    print(f"\nüîç METHOD 1: Absolute threshold detection")
    print(f"   Threshold: {threshold}")
    
    start_time = time.time()
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise Exception(f"Could not open video: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"üìä Processing {total_frames:,} frames at {fps:.2f} fps...")
    
    cut_points = [0.0]  # Always include start
    prev_hash = None
    frame_count = 0
    similarity_values = []
    
    # Process frames individually for better progress tracking
    last_progress_time = time.time()
    cuts_found = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print(f"   üìπ Reached end of video at frame {frame_count}")
            break
        
        current_hash = enhanced_perceptual_hash(frame)
        
        if prev_hash is not None:
            similarity, basic_sim, edge_sim, hist_sim = compare_enhanced_frames(prev_hash, current_hash)
            similarity_values.append(similarity)
            
            if similarity < threshold:
                timestamp = frame_count / fps
                # Avoid cuts too close together
                if not cut_points or (timestamp - cut_points[-1]) > 1.0:
                    cut_points.append(timestamp)
                    cuts_found += 1
                    print(f"üé¨ CUT {cuts_found} at {timedelta(seconds=int(timestamp))} (similarity: {similarity:.3f})")
                    print(f"   Frame {frame_count:,} | Basic: {basic_sim:.3f} | Edge: {edge_sim:.3f} | Hist: {hist_sim:.3f}")
        
        prev_hash = current_hash
        frame_count += 1
        
        # Progress indicator with more details
        current_time = time.time()
        if frame_count % 500 == 0 or current_time - last_progress_time > 3.0:
            progress = (frame_count / total_frames) * 100
            elapsed = current_time - start_time
            fps_analysis = frame_count / elapsed if elapsed > 0 else 0
            eta_seconds = (total_frames - frame_count) / fps_analysis if fps_analysis > 0 else 0
            
            print(f"   Progress: {progress:.1f}% ({frame_count:,}/{total_frames:,}) | {fps_analysis:.1f} fps | ETA: {int(eta_seconds)}s | Cuts: {cuts_found}")
            last_progress_time = current_time
    
    cap.release()
    
    # Add end time
    duration = total_frames / fps
    cut_points.append(duration)
    
    elapsed = time.time() - start_time
    print(f"‚úÖ Method 1 complete in {elapsed:.1f}s")
    print(f"   Result: {len(cut_points) - 1} segments with {len(cut_points) - 2} cut points")
    
    # Show similarity statistics
    if similarity_values:
        print(f"üìä Similarity stats: min={np.min(similarity_values):.4f}, max={np.max(similarity_values):.4f}, mean={np.mean(similarity_values):.4f}")
    
    return cut_points, fps, similarity_values

def extract_boundary_frames_optimized(video_path, cut_points, fps):
    """Extract frames at segment boundaries with better sampling"""
    print(f"\nüéûÔ∏è  Extracting boundary frames from {os.path.basename(video_path)}...")
    print(f"   Processing {len(cut_points) - 1} segments")
    
    start_time = time.time()
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise Exception(f"Could not open video: {video_path}")
    
    boundary_frames = []
    
    # Extract multiple frames per boundary for better matching
    for i in range(len(cut_points) - 1):
        start_time_seg = cut_points[i]
        end_time_seg = cut_points[i + 1]
        segment_duration = end_time_seg - start_time_seg
        
        print(f"   Segment {i+1}: {timedelta(seconds=int(start_time_seg))} - {timedelta(seconds=int(end_time_seg))} ({segment_duration:.1f}s)")
        
        # Sample frames strategically
        sample_times = []
        
        # Always sample start
        sample_times.append(start_time_seg)
        
        # Sample a few frames into the segment
        if segment_duration > 5:
            sample_times.extend([
                start_time_seg + 1.0,
                start_time_seg + 2.0,
                max(start_time_seg + 3.0, end_time_seg - 2.0)
            ])
        elif segment_duration > 2:
            sample_times.append(start_time_seg + 1.0)
        
        # Sample end (but not too close to next segment)
        end_sample_time = max(start_time_seg + 0.5, end_time_seg - 0.5)
        sample_times.append(end_sample_time)
        
        # Extract frames
        for j, sample_time in enumerate(sample_times):
            cap.set(cv2.CAP_PROP_POS_MSEC, sample_time * 1000)
            ret, frame = cap.read()
            if ret:
                hash_val = enhanced_perceptual_hash(frame)
                frame_type = 'start' if j == 0 else ('end' if j == len(sample_times) - 1 else 'middle')
                boundary_frames.append({
                    'time': sample_time,
                    'hash': hash_val,
                    'type': frame_type,
                    'segment': i,
                    'sample_index': j
                })
                print(f"     Sample {j+1}: {timedelta(seconds=int(sample_time))} ({frame_type})")
    
    cap.release()
    
    elapsed = time.time() - start_time
    print(f"‚úÖ Extracted {len(boundary_frames)} boundary frames in {elapsed:.1f}s")
    return boundary_frames

def find_frames_in_original_optimized(original_path, boundary_frames, search_window=30):
    """Optimized frame search with progressive starting points and parallel processing"""
    print(f"\nüîç Searching for boundary frames in {os.path.basename(original_path)}...")
    
    start_time = time.time()
    
    cap = cv2.VideoCapture(original_path)
    if not cap.isOpened():
        raise Exception(f"Could not open video: {original_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    cap.release()
    
    print(f"üìä Original video: {duration:.1f}s ({total_frames:,} frames at {fps:.2f} fps)")
    
    # Group boundary frames by segment for better processing
    segments = {}
    for bf in boundary_frames:
        seg_id = bf['segment']
        if seg_id not in segments:
            segments[seg_id] = []
        segments[seg_id].append(bf)
    
    print(f"   Processing {len(segments)} segments with {len(boundary_frames)} boundary frames")
    
    matches = []
    last_found_time = 0.0  # Progressive search optimization
    
    # Process segments in order
    for seg_id in sorted(segments.keys()):
        segment_boundaries = segments[seg_id]
        print(f"\nüîç Processing segment {seg_id+1} with {len(segment_boundaries)} boundary frames...")
        print(f"   Starting search from {timedelta(seconds=int(last_found_time))}")
        
        # Find the best representative frame from this segment
        best_boundary = None
        best_match_score = 0
        best_match_time = last_found_time
        
        # Search for each boundary frame in this segment
        for i, boundary in enumerate(segment_boundaries):
            print(f"   Searching boundary frame {i+1}/{len(segment_boundaries)} ({boundary['type']})...")
            
            match_result = search_single_frame(
                original_path, boundary, last_found_time, duration, fps
            )
            
            if match_result and match_result['similarity'] > best_match_score:
                best_match_score = match_result['similarity']
                best_match_time = match_result['original_time']
                best_boundary = boundary
                print(f"     New best match: {timedelta(seconds=int(best_match_time))} (similarity: {best_match_score:.3f})")
        
        if best_boundary and best_match_score > 0.6:
            matches.append({
                'boundary': best_boundary,
                'original_time': best_match_time,
                'similarity': best_match_score,
                'segment': seg_id
            })
            
            # Update progressive search starting point
            last_found_time = best_match_time
            print(f"‚úÖ Segment {seg_id+1} matched at {timedelta(seconds=int(best_match_time))} (similarity: {best_match_score:.3f})")
        else:
            print(f"‚ö†Ô∏è  No good match for segment {seg_id+1} (best score: {best_match_score:.3f})")
            # Don't update last_found_time if no match
    
    elapsed = time.time() - start_time
    print(f"\n‚úÖ Frame matching complete in {elapsed:.1f}s")
    print(f"   Found {len(matches)} segment matches out of {len(segments)} segments")
    return matches

def search_single_frame(original_path, boundary, start_time, duration, fps, search_step=2.0):
    """Search for a single frame starting from a given time"""
    cap = cv2.VideoCapture(original_path)
    if not cap.isOpened():
        return None
    
    best_match = {
        'time': -1,
        'similarity': 0
    }
    
    searches_done = 0
    
    # Search from start_time forward, with fallback to full search if needed
    search_ranges = [
        (start_time, duration),  # Forward from last found
        (0, start_time) if start_time > 0 else None  # Backward fallback
    ]
    
    for search_start, search_end in filter(None, search_ranges):
        if search_start >= search_end:
            continue
            
        for search_time in np.arange(search_start, search_end, search_step):
            cap.set(cv2.CAP_PROP_POS_MSEC, search_time * 1000)
            ret, frame = cap.read()
            
            if not ret:
                continue
            
            original_hash = enhanced_perceptual_hash(frame)
            similarity, _, _, _ = compare_enhanced_frames(
                boundary['hash'], original_hash
            )
            
            if similarity > best_match['similarity']:
                best_match = {
                    'time': search_time,
                    'similarity': similarity,
                    'original_time': search_time
                }
            
            searches_done += 1
            
            # Progress update for long searches
            if searches_done % 100 == 0:
                print(f"     Searched {searches_done} positions, best: {best_match['similarity']:.3f}")
            
            # Early termination if we find a very good match
            if similarity > 0.9:
                print(f"     Found excellent match early (similarity: {similarity:.3f})")
                break
        
        # If we found a decent match in forward search, don't do backward
        if best_match['similarity'] > 0.7:
            break
    
    cap.release()
    
    if best_match['similarity'] > 0.3:
        print(f"     Match found: {timedelta(seconds=int(best_match['time']))} (similarity: {best_match['similarity']:.3f})")
    else:
        print(f"     No good match found (best: {best_match['similarity']:.3f})")
    
    return best_match if best_match['similarity'] > 0.3 else None

def reconstruct_segments_smart(matches):
    """Smart segment reconstruction with gap filling"""
    print(f"\nüîß Reconstructing segments from {len(matches)} matches...")
    
    if not matches:
        print("‚ùå No matches to reconstruct from")
        return []
    
    # Sort matches by original video time
    matches.sort(key=lambda x: x['original_time'])
    
    print("üìä Match timeline:")
    for i, match in enumerate(matches):
        print(f"   {i+1}. Segment {match['segment']+1} -> {timedelta(seconds=int(match['original_time']))} (sim: {match['similarity']:.3f})")
    
    segments = []
    
    # Simple approach: create segments from consecutive matches
    for i in range(len(matches) - 1):
        start_time = matches[i]['original_time']
        end_time = matches[i + 1]['original_time']
        
        # Only include if segment is reasonable length
        if end_time > start_time and (end_time - start_time) >= 2.0:
            segments.append((start_time, end_time))
            duration = end_time - start_time
            print(f"‚úÖ Segment {len(segments)}: {timedelta(seconds=int(start_time))} - {timedelta(seconds=int(end_time))} ({duration:.1f}s)")
        else:
            print(f"‚ö†Ô∏è  Skipping invalid segment: {start_time:.1f}s - {end_time:.1f}s")
    
    total_duration = sum(end - start for start, end in segments)
    print(f"\nüìä Final segments: {len(segments)} segments, {timedelta(seconds=int(total_duration))} total")
    
    return segments

def create_cut_video_optimized(input_path, segments, output_

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def detect_cut_points_adaptive(video_path, initial_threshold=0.03, max_segments=50):
    """Adaptive cut detection that adjusts threshold based on results"""
    print(f"Adaptive cut detection starting with threshold {initial_threshold}...")
    
    cap = cv2.VideoCapture(video_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    
    # First pass: collect similarity data with frame skipping for speed
    print("Quick analysis pass...")
    similarities = []
    frame_positions = []

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        if frame_count % 1000 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"\rQuick analysis: {progress:.1f}%", end="", flush=True)
    
    cap.release()

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    percentiles = [1, 3, 5, 10, 15, 20]
    
    print(f"\nSimilarity analysis:")
    for p in percentiles:
        thresh = np.percentile(similarities, p)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        thresh = np.percentile(similarities, p)
        potential_cuts = np.sum(similarities < thresh)
        print(f"  {p}th percentile: {thresh:.4f} ({potential_cuts} potential cuts)")
    
    # Choose threshold that gives reasonable number of cuts

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    
    # Second pass: detect cuts with chosen threshold
    print(f"\nDetecting cuts with threshold {chosen_threshold:.4f}...")
    return detect_cut_points_method1(video_path, chosen_threshold)


# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def detect_cut_points_method2(video_path, drop_threshold=0.15, base_threshold=0.05):
    """Method 2: Detect cuts using similarity drops and rolling average"""
    print(f"Method 2: Detecting cuts with drop threshold {drop_threshold} and base {base_threshold}...")
    
    cap = cv2.VideoCapture(video_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    hash_cache = {}
    
    print(f"Analyzing {total_frames} frames...")
    
    while True:

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                if not cut_points or (timestamp - cut_points[-1]) > 2.0:
                    cut_points.append(timestamp)
                    print(f"\nCut detected at {timedelta(seconds=int(timestamp))} - {cut_reason}")
            
            # Debug output

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            if frame_count % 1000 == 0:
                window_avg = np.mean(similarity_window) if similarity_window else 0
                print(f"\nFrame {frame_count}: sim={similarity:.3f}, window_avg={window_avg:.3f}, adaptive_thresh={adaptive_threshold:.3f}")
            
            prev_similarity = similarity

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        if frame_count % 500 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"\rProgress: {progress:.1f}%", end="", flush=True)
    
    cap.release()

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    cut_points.append(duration)
    
    print(f"\nMethod 2 result: {len(cut_points) - 1} segments with {len(cut_points) - 2} cut points")
    return cut_points, fps


# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def detect_cut_points_method3(video_path, percentile_threshold=5):
    """Method 3: Statistical approach - detect cuts based on percentile of all similarities"""
    print(f"Method 3: Statistical detection using {percentile_threshold}th percentile...")
    
    cap = cv2.VideoCapture(video_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    
    # First pass: collect all similarities
    print("First pass: collecting similarity data...")
    similarities = []
    timestamps = []

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        if frame_count % 1000 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"\rFirst pass progress: {progress:.1f}%", end="", flush=True)
    
    cap.release()

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    # Calculate statistical threshold
    threshold = np.percentile(similarities, percentile_threshold)
    print(f"\nCalculated threshold: {threshold:.3f} ({percentile_threshold}th percentile)")
    print(f"Similarity range: {np.min(similarities):.3f} to {np.max(similarities):.3f}")
    

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    threshold = np.percentile(similarities, percentile_threshold)
    print(f"\nCalculated threshold: {threshold:.3f} ({percentile_threshold}th percentile)")
    print(f"Similarity range: {np.min(similarities):.3f} to {np.max(similarities):.3f}")
    
    # Second pass: find cuts

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            if not cut_points or (timestamp - cut_points[-1]) > 2.0:
                cut_points.append(timestamp)
                print(f"Cut detected at {timedelta(seconds=int(timestamp))} (similarity: {sim:.3f})")
    
    # Add end time

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    cut_points.append(duration)
    
    print(f"\nMethod 3 result: {len(cut_points) - 1} segments with {len(cut_points) - 2} cut points")
    return cut_points, fps


# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def detect_cut_points_method1(video_path, threshold=0.03):
    """Method 1: Detect cut points using absolute similarity threshold (optimized)"""
    print(f"Method 1: Detecting cuts with threshold {threshold}...")
    
    cap = cv2.VideoCapture(video_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    frame_batch = []
    
    print(f"Analyzing {total_frames} frames...")
    
    while True:

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                        if not cut_points or (timestamp - cut_points[-1]) > 1.0:
                            cut_points.append(timestamp)
                            print(f"\nCut detected at {timedelta(seconds=int(timestamp))} (similarity: {similarity:.3f})")
                
                prev_hash = current_hash

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            if frame_count % 1000 == 0:
                progress = (frame_count / total_frames) * 100
                print(f"\rProgress: {progress:.1f}%", end="", flush=True)
        else:
            frame_count += 1

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    cut_points.append(duration)
    
    print(f"\nMethod 1 result: {len(cut_points) - 1} segments with {len(cut_points) - 2} cut points")
    return cut_points, fps, similarity_values


# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def extract_boundary_frames_optimized(video_path, cut_points, fps):
    """Extract frames at segment boundaries with better sampling"""
    print(f"Extracting boundary frames from {os.path.basename(video_path)}...")
    
    cap = cv2.VideoCapture(video_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    
    cap.release()
    print(f"Extracted {len(boundary_frames)} boundary frames with multiple samples per segment")
    return boundary_frames


# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def find_frames_in_original_optimized(original_path, boundary_frames, search_window=30):
    """Optimized frame search with progressive starting points and parallel processing"""
    print(f"Searching for boundary frames in {os.path.basename(original_path)}...")
    
    cap = cv2.VideoCapture(original_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    for seg_id in sorted(segments.keys()):
        segment_boundaries = segments[seg_id]
        print(f"\nProcessing segment {seg_id} with {len(segment_boundaries)} boundary frames...")
        
        # Find the best representative frame from this segment

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            # Update progressive search starting point
            last_found_time = best_match_time
            print(f"‚úÖ Segment {seg_id} matched at {timedelta(seconds=int(best_match_time))} (similarity: {best_match_score:.3f})")
        else:
            print(f"‚ö†Ô∏è  No good match for segment {seg_id}")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            print(f"‚úÖ Segment {seg_id} matched at {timedelta(seconds=int(best_match_time))} (similarity: {best_match_score:.3f})")
        else:
            print(f"‚ö†Ô∏è  No good match for segment {seg_id}")
            # Don't update last_found_time if no match
    

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            # Don't update last_found_time if no match
    
    print(f"\nFound {len(matches)} segment matches")
    return matches


# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def reconstruct_segments_smart(matches):
    """Smart segment reconstruction with gap filling"""
    print("\nReconstructing segments from matches...")
    
    if not matches:

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            segments.append((start_time, end_time))
            duration = end_time - start_time
            print(f"Segment {len(segments)}: {timedelta(seconds=int(start_time))} - {timedelta(seconds=int(end_time))} ({duration:.1f}s)")
    
    return segments

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def create_cut_video_optimized(input_path, segments, output_path):
    """Create cut video using ffmpeg with optimized settings"""
    print(f"\nCreating cut video: {os.path.basename(output_path)}")
    
    if not segments:

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    
    if not segments:
        print("‚ùå No segments to process")
        return False
    

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        ]
        
        print("Running optimized ffmpeg...")
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ Successfully created: {output_path}")
            return True
        else:

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            return True
        else:
            print(f"‚ùå FFmpeg error: {result.stderr}")
            # Fallback to re-encoding
            return create_cut_video_fallback(input_path, segments, output_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    
    except Exception as e:
        print(f"‚ùå Error with optimized method: {e}")
        return create_cut_video_fallback(input_path, segments, output_path)
    

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
def create_cut_video_fallback(input_path, segments, output_path):
    """Fallback method with re-encoding"""
    print("Using fallback method with re-encoding...")
    
    # Create filter for concatenating segments

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        return result.returncode == 0
    except FileNotFoundError:
        print("‚ùå FFmpeg not found. Please install ffmpeg first.")
        return False


# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    # Check if files exist
    if not os.path.exists(original_path):
        print(f"‚ùå Original video not found: {original_path}")
        return
    

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
    
    if not os.path.exists(edited_path):
        print(f"‚ùå Edited video not found: {edited_path}")
        return
    

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        install_requirements()
        
        print("üöÄ OPTIMIZED Multi-Method Cut Point Matcher")
        print("=" * 50)
        

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        
        print("üöÄ OPTIMIZED Multi-Method Cut Point Matcher")
        print("=" * 50)
        
        # Try all methods like the original, but with optimizations

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        
        # Try all methods like the original, but with optimizations
        print("\n" + "="*50)
        print("TRYING ADAPTIVE METHOD FIRST")
        print("="*50)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        # Try all methods like the original, but with optimizations
        print("\n" + "="*50)
        print("TRYING ADAPTIVE METHOD FIRST")
        print("="*50)
        

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        print("\n" + "="*50)
        print("TRYING ADAPTIVE METHOD FIRST")
        print("="*50)
        
        # First try adaptive detection

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        
        if len(cut_points) > 2:  # More than just start and end
            print(f"‚úÖ Adaptive method found {len(cut_points) - 1} segments!")
            success_method = "Adaptive"
            final_cut_points = cut_points

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            final_fps = fps
        else:
            print("‚ùå Adaptive method didn't find enough cuts, trying Method 2...")
            
            # Try Method 2

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            
            # Try Method 2
            print("\n" + "="*50)
            print("TRYING METHOD 2: Similarity Drops")
            print("="*50)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            # Try Method 2
            print("\n" + "="*50)
            print("TRYING METHOD 2: Similarity Drops")
            print("="*50)
            cut_points_2, fps_2 = detect_cut_points_method2(edited_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            print("\n" + "="*50)
            print("TRYING METHOD 2: Similarity Drops")
            print("="*50)
            cut_points_2, fps_2 = detect_cut_points_method2(edited_path)
            

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            
            if len(cut_points_2) > 2:
                print(f"‚úÖ Method 2 found {len(cut_points_2) - 1} segments!")
                success_method = 2
                final_cut_points = cut_points_2

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                final_fps = fps_2
            else:
                print("‚ùå Method 2 didn't find enough cuts, trying Method 3...")
                
                # Try Method 3

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                
                # Try Method 3
                print("\n" + "="*50)
                print("TRYING METHOD 3: Statistical Detection")
                print("="*50)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                # Try Method 3
                print("\n" + "="*50)
                print("TRYING METHOD 3: Statistical Detection")
                print("="*50)
                cut_points_3, fps_3 = detect_cut_points_method3(edited_path)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                print("\n" + "="*50)
                print("TRYING METHOD 3: Statistical Detection")
                print("="*50)
                cut_points_3, fps_3 = detect_cut_points_method3(edited_path)
                

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                
                if len(cut_points_3) > 2:
                    print(f"‚úÖ Method 3 found {len(cut_points_3) - 1} segments!")
                    success_method = 3
                    final_cut_points = cut_points_3

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                    final_fps = fps_3
                else:
                    print("‚ùå All methods failed to detect cuts.")
                    
                    # Show similarity statistics for manual adjustment

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                    # Show similarity statistics for manual adjustment
                    if similarities:
                        print(f"\nüìä SIMILARITY STATISTICS:")
                        print(f"Min similarity: {np.min(similarities):.4f}")
                        print(f"5th percentile: {np.percentile(similarities, 5):.4f}")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                    if similarities:
                        print(f"\nüìä SIMILARITY STATISTICS:")
                        print(f"Min similarity: {np.min(similarities):.4f}")
                        print(f"5th percentile: {np.percentile(similarities, 5):.4f}")
                        print(f"Average: {np.mean(similarities):.4f}")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                        print(f"\nüìä SIMILARITY STATISTICS:")
                        print(f"Min similarity: {np.min(similarities):.4f}")
                        print(f"5th percentile: {np.percentile(similarities, 5):.4f}")
                        print(f"Average: {np.mean(similarities):.4f}")
                        print(f"Suggested thresholds to try:")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                        print(f"Min similarity: {np.min(similarities):.4f}")
                        print(f"5th percentile: {np.percentile(similarities, 5):.4f}")
                        print(f"Average: {np.mean(similarities):.4f}")
                        print(f"Suggested thresholds to try:")
                        for p in [1, 3, 5, 10]:

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                        print(f"5th percentile: {np.percentile(similarities, 5):.4f}")
                        print(f"Average: {np.mean(similarities):.4f}")
                        print(f"Suggested thresholds to try:")
                        for p in [1, 3, 5, 10]:
                            thresh = np.percentile(similarities, p)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                        for p in [1, 3, 5, 10]:
                            thresh = np.percentile(similarities, p)
                            print(f"- {p}th percentile: {thresh:.4f}")
                    return
        

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        
        if success_method and final_cut_points:
            print(f"\n" + "="*50)
            print(f"PROCESSING WITH METHOD {success_method}")
            print("="*50)

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        if success_method and final_cut_points:
            print(f"\n" + "="*50)
            print(f"PROCESSING WITH METHOD {success_method}")
            print("="*50)
            

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
            print(f"\n" + "="*50)
            print(f"PROCESSING WITH METHOD {success_method}")
            print("="*50)
            
            # Extract boundary frames with better sampling

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                # Calculate total duration
                total_kept = sum(end - start for start, end in segments)
                print(f"\nFinal result:")
                print(f"Method used: {success_method}")
                print(f"Segments found: {len(segments)}")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                total_kept = sum(end - start for start, end in segments)
                print(f"\nFinal result:")
                print(f"Method used: {success_method}")
                print(f"Segments found: {len(segments)}")
                print(f"Total duration to keep: {timedelta(seconds=int(total_kept))}")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                print(f"\nFinal result:")
                print(f"Method used: {success_method}")
                print(f"Segments found: {len(segments)}")
                print(f"Total duration to keep: {timedelta(seconds=int(total_kept))}")
                

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                print(f"Method used: {success_method}")
                print(f"Segments found: {len(segments)}")
                print(f"Total duration to keep: {timedelta(seconds=int(total_kept))}")
                
                # Create the cut video with optimizations

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                # Create the cut video with optimizations
                if create_cut_video_optimized(original_path, segments, output_path):
                    print(f"\nüéâ SUCCESS! Optimized cut video saved as: {output_path}")
                else:
                    print("\n‚ùå Failed to create cut video")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                    print(f"\nüéâ SUCCESS! Optimized cut video saved as: {output_path}")
                else:
                    print("\n‚ùå Failed to create cut video")
            else:
                print("\n‚ùå No valid segments found after matching")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                    print("\n‚ùå Failed to create cut video")
            else:
                print("\n‚ùå No valid segments found after matching")
        else:
            print("\n‚ùå No cuts detected in the edited video")

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
                print("\n‚ùå No valid segments found after matching")
        else:
            print("\n‚ùå No cuts detected in the edited video")
        
    except Exception as e:

# [VERBOSE_PATCHED] Added or restored from video_cut_transfer.py
        
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()